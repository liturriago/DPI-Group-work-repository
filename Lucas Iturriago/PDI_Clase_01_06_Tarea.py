# -*- coding: utf-8 -*-
"""PDI - Clase 01/06 - Tarea.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BNqJPTw-taFbdIGlNt8v_NOuY787-t8e

## Consultar las métricas de desempeño de huber y área bajo la curva ROC(ecuación, ventajas y desventajas).

Si el AUC se encuentra comprendido entre 0.5 y 0.7 una clasificación mala o una baja exactitud en la prueba, si este se encuentra entre 0.7 y 0.9 decimos que es una exactitud regular-alta dependiendo lo que estemos trabajando y si el AUC es superior a 0.9 entonces decimos que tiene una exactitud alta.

Desventajas:

* No se muestran los puntos de corte, sólo se muestran su sensibilidad y especificidad asociadas.

* No se muestra el número de sujetos.

* Si se disminuye el tamaño de la muestra, la curva tiende a hacerse más escalonada.

Ventajas:

* Es una representación fácilmente comprensible hablando de forma cualitativa podemos decir si el clasifcador lo está haciendo bien o no.

* No requieren un nivel de decisión particular porque comprende todo el rango posible de valores.

## Consultar cómo se generalizan las medidas de desempeño para clasificación binaria anteriormente vistas a tareas multi-clase y multi-salida.

Para generalizar las medidas de desempeño la clasificación binaria a tareas multi-clase o multi-salida como f1-score, curva roc o alguna otra métrica binaria, se debe colocar dentro de los parametros "average" sabiendo que se puede usar en esa medida; los valores que puede tomar el parametro "average" son:

* "macro" cálcula la media de las métricas binarias, otorgando el mismo peso a cada clase. En problemas donde las clases poco frecuentes son importantes, el macropromedio puede ser un medio para resaltar su desempeño. Por otro lado, la suposición de que todas las clases son igualmente importantes a menudo es falsa, de modo que el macropromedio enfatizará demasiado el rendimiento típicamente bajo en una clase poco frecuente.

* "weighted" da cuenta del desequilibrio de clase calculando el promedio de métricas binarias en las que la puntuación de cada clase se pondera por su presencia en la muestra de datos reales.

* "micro" da a cada par de clase de muestra una contribución igual a la métrica general (excepto como resultado del peso de la muestra). En lugar de sumar la métrica por clase, esto suma los dividendos y divisores que componen las métricas por clase para calcular un cociente general. Se puede preferir el micropromedio en configuraciones de múltiples etiquetas, incluida la clasificación de múltiples clases donde se debe ignorar una clase mayoritaria.

(Seleccionar average=None devolverá una matriz con la puntuación de cada clase)

## Crear una clase que permita evaluar las métricas tratadas en clase y las de la consulta. Para el caso de clasificación debe soportar tareas binarias, multi-clase y multi-salida, teniendo en cuenta todos los parámetros necesarios para su cálculo como sus validaciones.

### Librerias
"""

import os
import pandas as pd
import numpy as np
from numpy import array
import tensorflow as tf
from sklearn.metrics import f1_score
from sklearn.metrics.cluster import completeness_score
from keras.models import Sequential
from keras.layers import Dense
from keras import backend
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import roc_curve, auc, roc_auc_score

"""###Clase"""

class metrica():
  def __init__(self):
      self.cont=0;

  def exactitud_b(self,y,y1):
    self.cont=0
    for i in range (0,len(y)):
      if (y[i]==y1[i]):
        self.cont=self.cont+1
    return self.cont/len(y)

  def clases(self,y):
    l=[y[0]]
    x=np.asarray(y)
    for i in range (0,len(x)):
      for j in range (0,len(l)):
        t=np.asarray(l)
        h=t[t!=x[i]]
        if (len(l)==len(h)):
          l.append(x[i])
    l=np.sort(l,axis=-1)
    return l

  def m_confusion(self,y,y1):
    fig=plt.figure()
    l=self.clases(y)
    y=np.asarray(y)
    y1=np.asarray(y1)
    z=np.zeros((len(l),len(l)))
    for i in range (0,len(l)):
      s=np.where(y==l[i])
      s=np.asarray(s)
      for j in range(0,s.shape[1]):
        for t in range (0,len(l)):
          if (y1[s[0,j]]==l[t]):
            z[t,i]=z[t,i]+1 
    z=np.transpose(z)       
    return z

  def plot_z(self,z,y,b):
    if (b==1):
      y=np.asarray(np.where(y==clase,0,1))
    l=self.clases(y)
    plt.imshow(z)     
    plt.colorbar()
    plt.xlabel("verdadera clase")
    plt.ylabel("prediccion de clase")
    plt.xticks(l,l)
    w=[]
    for i in range (0,len(l)):
      w.append(l[len(l)-i-1])
    plt.yticks(w,w)
  
  def precision(self,y,y1,clase):
    y=np.asarray(np.where(y==clase,0,1))
    y1=np.asarray(np.where(y1==clase,0,1))
    z=self.m_confusion(y,y1)
    p=np.zeros((z.shape[0]))
    for i in range (0,z.shape[0]):
      vp=z[i,i]
      vfp=np.sum(z[:,i],axis=0)
      p[i]=vp/vfp
    return p

  def exahustividad(self,y,y1,clase):
    y=np.asarray(np.where(y==clase,0,1))
    y1=np.asarray(np.where(y1==clase,0,1))
    z=self.m_confusion(y,y1)
    p=np.zeros((z.shape[0]))
    for i in range (0,z.shape[0]):
      vp=z[i,i]
      vfp=np.sum(z[i,:],axis=0)
      p[i]=vp/vfp
    return p

"""### Ejemplos"""

prueba=metrica()

x_real=np.asarray([0,0,0,1,1,1,0,1,1,1])#,3,3,2,2])
X_pred=np.asarray([0,0,1,1,1,0,0,1,1,1])#,3,3,2,2])
prueba.clases(x_real)

e=prueba.exactitud_b(x_real,X_pred)
matriz=prueba.m_confusion(x_real,X_pred)
print(e,matriz)
prueba.plot_z(matriz,x_real,2)

preci=prueba.precision(x_real,X_pred,1)
exha=prueba.exahustividad(x_real,X_pred,1)
print(preci, exha)

prueba=metrica()

x_real=np.asarray([0,0,0,1,3,1,0,1,1,1,3,3,2,2])
X_pred=np.asarray([0,0,1,2,1,0,3,1,1,1,3,3,2,2])
prueba.clases(x_real)

e=prueba.exactitud_b(x_real,X_pred)
matriz=prueba.m_confusion(x_real,X_pred)
print(e,matriz)
prueba.plot_z(matriz,x_real,2)

preci=prueba.precision(x_real,X_pred,1)
exha=prueba.exahustividad(x_real,X_pred,1)
print(preci, exha)

"""## Replicar la clase del apartado anterior utilizando los paquetes de Sklearn y Keras

### Clase
"""

class metrics():

  def __init__(self, y, yp):

    self.y = y
    self.yp = yp

  def metric1(self):

    RMSE = tf.keras.metrics.RootMeanSquaredError()
    RMSE.update_state([self.y], [self.yp])

    return RMSE.result().numpy()

  def metric2(self):
    MAE = tf.keras.metrics.MeanAbsoluteError()
    MAE.update_state([self.y], [self.yp])

    return MAE.result().numpy()

  def metric3(self):
    CM = confusion_matrix(self.y, self.yp)

    return CM

  def metric4(self):
    AC = tf.keras.metrics.Accuracy()
    AC.update_state([self.y], [self.yp])

    return AC.result().numpy()

  def metric5(self):
    CS = completeness_score(self.y, self.yp)
    return CS
  
  def metric6(self):
    F1S = f1_score(self.y, self.yp, average='macro')

    return F1S

  def metric7(self):
    F1SP = f1_score(self.y, self.yp, average='weighted')

    return F1SP
  
  def metric8(self):
    fpr,tpr,thr=roc_curve(self.y,self.yp)
    p_curve=(plt.plot(fpr,tpr,label="ROC curve (area = %0.2f)" %roc_auc_score(self.y,self.yp,average=None)),plt.plot(tpr,tpr,'g--'),plt.legend())
    auc=print("El área bajo la curva roc es:", roc_auc_score(self.y,self.yp,average=None))
    
    return p_curve,auc

"""### Ejemplos"""

y_b = [1, 1, 0, 1, 1, 1, 1, 0, 1, 0]
yp_b = [0, 1, 1, 0, 0, 0, 0, 1, 1,0]

y_m = [2, 0, 2, 1, 1, 0, 1, 2, 1, 2]
yp_m = [1, 0, 0, 2, 1, 1, 2, 0, 2, 1]

l = metrics(y_b,yp_b)
l.metric8()

l = metrics(y_m,yp_m)
l.metric7()